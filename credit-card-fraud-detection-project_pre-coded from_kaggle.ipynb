{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":23498,"sourceType":"datasetVersion","datasetId":310}],"dockerImageVersionId":31192,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-25T22:50:55.033025Z","iopub.execute_input":"2025-11-25T22:50:55.033279Z","iopub.status.idle":"2025-11-25T22:50:57.468Z","shell.execute_reply.started":"2025-11-25T22:50:55.033248Z","shell.execute_reply":"2025-11-25T22:50:57.467061Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"\n# **1.INTRODUCTION: CREDIT CARD FRAUD DETECTION**\n\nThe goal of this project is to build an end-to-end machine learning pipeline that can automatically detect fraudulent credit card transactions. I chose the Credit Card Fraud Detection dataset from Kaggle, which contains real-world transaction data collected by a European bank.\n\nThe task is a binary classification problem, where the model must predict whether a transaction is:\n\n0 → Legitimate\n\n1 → Fraudulent\n\nThe objective is to build a model that can detect fraud with the highest possible recall and F1-score for the minority class (fraud), while following machine learning theory and justifying each step in the pipeline.\n\nIn this project, I followed the same pipeline we learned from the Titanic example.","metadata":{}},{"cell_type":"markdown","source":"# 2. 1. DATA LOADING <a id=\"data-loading-eda\"></a>\n## 2.1 Importing the Dataset","metadata":{}},{"cell_type":"code","source":"# 1. DATA LOADING <a id=\"data-loading-eda\"></a>\n# 1.1 Importing the Dataset\nimport pandas as pd\n\ndf = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:21:19.249934Z","iopub.execute_input":"2025-11-26T00:21:19.251273Z","iopub.status.idle":"2025-11-26T00:21:22.048013Z","shell.execute_reply.started":"2025-11-26T00:21:19.251237Z","shell.execute_reply":"2025-11-26T00:21:22.046433Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"# 3.DATA CLEANING\n\n## 3.1 Quick overview : verifying the dataset and that the target column (Class) is present","metadata":{}},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"# 3.DATA CLEANING\n\n# 3.1 Quick overview : verifying the dataset and that the target column (Class) is present\n\nimport pandas as pd\ndf = pd.read_csv('/kaggle/input/creditcardfraud/creditcard.csv')  # your file path\nprint(\"Shape:\", df.shape)\nprint(\"\\nColumns:\\n\", df.columns.tolist())\nprint(\"\\nInfo:\")\nprint(df.info())\nprint(\"\\nFirst 5 rows:\")\ndisplay(df.head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:21:23.219433Z","iopub.execute_input":"2025-11-26T00:21:23.219821Z","iopub.status.idle":"2025-11-26T00:21:26.005382Z","shell.execute_reply.started":"2025-11-26T00:21:23.219793Z","shell.execute_reply":"2025-11-26T00:21:26.004376Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Summary and missing values","metadata":{}},{"cell_type":"code","source":"# 3.2 Summary and missing values\n\n# Here, i am checking the missing data and getting the sense of distributions, ranges, skewness.\n# If missing values exist — description of chosen imputation strategy (mean/median/mode/model-based) is necessary and why (median for skewed data, mean for symmetric). If none, the data is complete.\n#No missing values as all columns shows zero (0)\n\nprint(\"Missing values per column:\")\nprint(df.isnull().sum())\nprint(\"\\nDescriptive statistics (numeric):\")\ndisplay(df.describe().T)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:21:26.402173Z","iopub.execute_input":"2025-11-26T00:21:26.403515Z","iopub.status.idle":"2025-11-26T00:21:26.940241Z","shell.execute_reply.started":"2025-11-26T00:21:26.403446Z","shell.execute_reply":"2025-11-26T00:21:26.939092Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.3 Checking Duplicates","metadata":{}},{"cell_type":"code","source":"#3.3 Checking Duplicates\nn_dup = df.duplicated().sum()\nprint(f\"Number of duplicate rows: {n_dup}\")\n\nif n_dup>0:\n    display(df[df.duplicated()].head())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:21:31.372796Z","iopub.execute_input":"2025-11-26T00:21:31.373152Z","iopub.status.idle":"2025-11-26T00:21:33.27579Z","shell.execute_reply.started":"2025-11-26T00:21:31.373127Z","shell.execute_reply":"2025-11-26T00:21:33.274249Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.4 Dropping duplicates","metadata":{}},{"cell_type":"code","source":"# 2.4 Dropping duplicates\n# duplicate causes the model to ‘see’ the same record multiple times, which can bias the learned patterns.\n\ndf = df.drop_duplicates()\ndf.shape\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:21:36.417198Z","iopub.execute_input":"2025-11-26T00:21:36.417509Z","iopub.status.idle":"2025-11-26T00:21:37.402831Z","shell.execute_reply.started":"2025-11-26T00:21:36.417488Z","shell.execute_reply":"2025-11-26T00:21:37.401565Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 4 EXPLORATORY DATA ANALYSIS (EDA)\n\n## 4.1 Class distribution","metadata":{}},{"cell_type":"code","source":"#4 EXPLORATORY DATA ANALYSIS (EDA)\n\n# 4.1 Class distribution\n#Fraud datasets are heavily imbalanced — this determines evaluation metrics and sampling strategy\n\nclass_counts = df['Class'].value_counts()\nclass_props = df['Class'].value_counts(normalize=True)\nprint(\"Counts:\\n\", class_counts)\nprint(\"\\nProportions:\\n\", class_props)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:21:40.299414Z","iopub.execute_input":"2025-11-26T00:21:40.299752Z","iopub.status.idle":"2025-11-26T00:21:40.312964Z","shell.execute_reply.started":"2025-11-26T00:21:40.29973Z","shell.execute_reply":"2025-11-26T00:21:40.311527Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 3.2 Simple plots","metadata":{}},{"cell_type":"code","source":"# 4.2 Simple plots\n# Spot skew and potential transformations (log), and class separation by amount/time.\n\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(12,4))\n\nplt.subplot(1,2,1)\nplt.hist(df['Amount'], bins=80)\nplt.title('Transaction Amount distribution')\nplt.xlabel('Amount')\n\nplt.subplot(1,2,2)\nplt.hist(df['Time'], bins=80)\nplt.title('Transaction Time distribution (seconds since start)')\nplt.xlabel('Time')\n\nplt.tight_layout()\nplt.show()\n\n# Boxplot of Amount by Class (fraud vs non-fraud)\nplt.figure(figsize=(6,4))\ndf.boxplot(column='Amount', by='Class', showfliers=False)\nplt.title('Amount by Class (without outliers displayed)')\nplt.suptitle('')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:21:44.84722Z","iopub.execute_input":"2025-11-26T00:21:44.847533Z","iopub.status.idle":"2025-11-26T00:21:46.08451Z","shell.execute_reply.started":"2025-11-26T00:21:44.847514Z","shell.execute_reply":"2025-11-26T00:21:46.083227Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 4.3 Checking correlations (quick heatmap or top correlated features with target)","metadata":{}},{"cell_type":"code","source":"#4.3 Checking correlations (quick heatmap or top correlated features with target)\n# Correlation with target and quick heatmap\n# To Understand which features relate to the target; PCA features may be anonymized (V1..V28) so interpret with caution\nimport seaborn as sns\ncorr_with_target = df.corr()['Class'].sort_values(ascending=False)\nprint(\"Top correlations with Class:\\n\", corr_with_target.head(10))\nprint(\"\\nBottom correlations with Class:\\n\", corr_with_target.tail(10))\n\n# Small heatmap of correlations for the top features by magnitude\ntop_feats = corr_with_target.abs().sort_values(ascending=False).head(10).index.tolist()\nplt.figure(figsize=(8,6))\nsns.heatmap(df[top_feats].corr(), annot=True, fmt='.2f', cmap='coolwarm')\nplt.title('Correlation heatmap (top features by |corr| with Class)')\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:21:49.651274Z","iopub.execute_input":"2025-11-26T00:21:49.651638Z","iopub.status.idle":"2025-11-26T00:21:51.088329Z","shell.execute_reply.started":"2025-11-26T00:21:49.651606Z","shell.execute_reply":"2025-11-26T00:21:51.087116Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 5. FEATURE ENGINEERING\n\n## 5.1 Train/test split","metadata":{}},{"cell_type":"code","source":"# 5. FEATURE ENGINEERING\n\n# 5.1 Train/test split\n# Reserve unseen data to evaluate final model. Use stratify because of imbalance.\n\n\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('Class', axis=1)\ny = df['Class']\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.2, random_state=42, stratify=y\n)\n\nprint(\"Train shape:\", X_train.shape, \"Test shape:\", X_test.shape)\nprint(\"Train class distribution:\\n\", y_train.value_counts(normalize=True))\nprint(\"Test class distribution:\\n\", y_test.value_counts(normalize=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:21:55.941981Z","iopub.execute_input":"2025-11-26T00:21:55.94227Z","iopub.status.idle":"2025-11-26T00:21:56.166764Z","shell.execute_reply.started":"2025-11-26T00:21:55.94225Z","shell.execute_reply":"2025-11-26T00:21:56.165633Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 5.2 Feature Scaling - Time and Amount","metadata":{}},{"cell_type":"code","source":"# 5.2 Feature Scaling - Time and Amount\n\n# Credit card dataset uses PCA features (V1–V28) which are already scaled but amount and time are NOT scaled → scaling helps most ML algorithms\n\n\nfrom sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler()\n\n# Copy data to avoid overwriting originals\nX_train_scaled = X_train.copy()\nX_test_scaled = X_test.copy()\n\n# Fit on train, transform both\nX_train_scaled[['Time', 'Amount']] = scaler.fit_transform(X_train[['Time', 'Amount']])\nX_test_scaled[['Time', 'Amount']] = scaler.transform(X_test[['Time', 'Amount']])\n\nX_train_scaled.head()\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:00.247374Z","iopub.execute_input":"2025-11-26T00:22:00.247748Z","iopub.status.idle":"2025-11-26T00:22:00.315371Z","shell.execute_reply.started":"2025-11-26T00:22:00.247722Z","shell.execute_reply":"2025-11-26T00:22:00.314069Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 6. DATA PREPROCESSING FOR MODELING\n## 6.1 Train Logistic Regression","metadata":{}},{"cell_type":"code","source":"# 6. DATA PREPROCESSING FOR MODELING\n# 6.1 Train Logistic Regression\n# Here, i am using class_weight='balanced' because fraud is extremely rare.\n# Balancing class weights give higher penalty to misclassifying the minority (fraud) class\n\nfrom sklearn.linear_model import LogisticRegression\n\nlog_reg = LogisticRegression(max_iter=2000, class_weight='balanced')\n\nlog_reg.fit(X_train_scaled, y_train)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:04.109292Z","iopub.execute_input":"2025-11-26T00:22:04.109623Z","iopub.status.idle":"2025-11-26T00:22:06.85519Z","shell.execute_reply.started":"2025-11-26T00:22:04.109599Z","shell.execute_reply":"2025-11-26T00:22:06.853907Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# 7.MODEL TRAINING, HYPERPARAMETER TUNING AND MODEL VALIDATION / EVALUATION\n\n## 7.1 confusion matrix, precision, recall, F1-score and ROC-AUC","metadata":{}},{"cell_type":"code","source":"# 7.MODEL TRAINING, HYPERPARAMETER TUNING AND MODEL VALIDATION / EVALUATION\n\n# 7.1 confusion matrix, precision, recall, F1-score and ROC-AUC\n\n# These are the correct metrics for imbalanced datasets (NOT accuracy)\n# Accuracy is misleading because non-fraud dominates; Recall is crucial; we want to catch as many frauds as possible; Precision matters to avoid too many false alarms; AUC measures ranking ability across thresholds.\n\nfrom sklearn.metrics import (\n    confusion_matrix, classification_report, roc_auc_score\n)\n\n# Predictions\ny_pred = log_reg.predict(X_test_scaled)\ny_prob = log_reg.predict_proba(X_test_scaled)[:, 1]\n\n# Confusion Matrix\nprint(\"Confusion Matrix:\")\nprint(confusion_matrix(y_test, y_pred))\n\n# Classification Report\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, y_pred))\n\n# ROC-AUC\nauc = roc_auc_score(y_test, y_prob)\nprint(\"\\nROC-AUC Score:\", auc)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:09.018035Z","iopub.execute_input":"2025-11-26T00:22:09.018389Z","iopub.status.idle":"2025-11-26T00:22:09.140876Z","shell.execute_reply.started":"2025-11-26T00:22:09.018362Z","shell.execute_reply":"2025-11-26T00:22:09.139602Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Explanation:\n# TN = 55,478 → model correctly said \"not fraud\" \n# TP = 90 → model correctly caught 90 fraud cases\n# FN = 8 → model missed 8 frauds → VERY GOOD (low FN)\n# FP = 1,386 → model wrongly flagged 1,386 normal transactions as fraud\n# The model catches fraud cases extremely well (high recall). But it produces many false alarms (high false positives)\n\n# In summary, our baseline Logistic Regression with class_weight='balanced' performs very well in terms of fraud detection (Recall = 0.92). This is crucial because in fraud detection missing a fraud is much more costly than flagging a normal transaction incorrectly. However, the model has low precision (0.06), meaning it produces many false positives. This is expected in highly imbalanced problems. The high ROC-AUC (0.97) indicates that the model separates fraud from non-fraud effectively, and we can further improve precision using more advanced models or threshold tuning.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:17.380783Z","iopub.execute_input":"2025-11-26T00:22:17.381178Z","iopub.status.idle":"2025-11-26T00:22:17.385543Z","shell.execute_reply.started":"2025-11-26T00:22:17.381151Z","shell.execute_reply":"2025-11-26T00:22:17.384644Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.2 Decision Threshold Tuning","metadata":{}},{"cell_type":"code","source":"# 7.2 Decision Threshold Tuning\n# Getting predicted probabilities (fraud probability)\n\ny_prob = log_reg.predict_proba(X_test_scaled)[:, 1]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:20.687326Z","iopub.execute_input":"2025-11-26T00:22:20.688532Z","iopub.status.idle":"2025-11-26T00:22:20.70219Z","shell.execute_reply.started":"2025-11-26T00:22:20.688486Z","shell.execute_reply":"2025-11-26T00:22:20.700948Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.3 Precision recall Curve","metadata":{}},{"cell_type":"code","source":"#7.3 Precision recall Curve\n\n# Plotting Precision-Recall curve\n# This curve is the most important tool for choosing a threshold in imbalanced datasets.\n\nfrom sklearn.metrics import precision_recall_curve\nimport matplotlib.pyplot as plt\n\nprecision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n\nplt.figure(figsize=(7,5))\nplt.plot(recall, precision)\nplt.xlabel(\"Recall\")\nplt.ylabel(\"Precision\")\nplt.title(\"Precision-Recall Curve\")\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:24.860678Z","iopub.execute_input":"2025-11-26T00:22:24.861019Z","iopub.status.idle":"2025-11-26T00:22:25.076927Z","shell.execute_reply.started":"2025-11-26T00:22:24.860995Z","shell.execute_reply":"2025-11-26T00:22:25.075622Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.4 Finding the Best Threshold","metadata":{}},{"cell_type":"code","source":"#7.4 Finding the Best Threshold\n# Normally we choose a point that balances both (recall and precision). But The default threshold (0.5) is not used in fraud detection\n# Hence, I will find the threshold that gives the best F1-score for the fraud class using this:\n\nimport numpy as np\nfrom sklearn.metrics import f1_score\n\nthresholds = np.linspace(0, 1, 101)  # 0.00, 0.01, ..., 1.00\n\nbest_threshold = 0\nbest_f1 = 0\n\nfor t in thresholds:\n    preds = (y_prob >= t).astype(int)\n    f1 = f1_score(y_test, preds)\n\n    if f1 > best_f1:\n        best_f1 = f1\n        best_threshold = t\n\nprint(\"Best Threshold:\", best_threshold)\nprint(\"Best F1 Score:\", best_f1)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:28.837155Z","iopub.execute_input":"2025-11-26T00:22:28.837508Z","iopub.status.idle":"2025-11-26T00:22:30.560699Z","shell.execute_reply.started":"2025-11-26T00:22:28.83745Z","shell.execute_reply":"2025-11-26T00:22:30.559213Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.5 Re-Evaluation of the Model with the New Threshold","metadata":{}},{"cell_type":"code","source":"# 7.5 Re-Evaluation of the Model with the New Threshold\n# This is to increase the precision (fraud), Fscore, and reduce false positive although recall (fraud) might drop slightly\n\n\noptimal_pred = (y_prob >= best_threshold).astype(int)\n\nprint(\"Confusion Matrix (optimal threshold):\")\nprint(confusion_matrix(y_test, optimal_pred))\n\nprint(\"\\nClassification Report:\")\nprint(classification_report(y_test, optimal_pred))\n\nprint(\"\\nROC-AUC Score (unchanged):\", roc_auc_score(y_test, y_prob))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:33.307366Z","iopub.execute_input":"2025-11-26T00:22:33.307716Z","iopub.status.idle":"2025-11-26T00:22:33.411398Z","shell.execute_reply.started":"2025-11-26T00:22:33.307691Z","shell.execute_reply":"2025-11-26T00:22:33.410225Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We tuned the threshold using the Precision–Recall curve to improve the model’s precision while maintaining a high recall. This reduces false positives and improves the model’s usefulness in real-world fraud detection.","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.6 Checking future importance","metadata":{}},{"cell_type":"code","source":"#7.6 Checking future importance\n#  This is to understand which features are influencing predictions, even if the model performs well.\n\n# Get feature importance from logistic regression coefficients\nimportance = pd.DataFrame({\n    'feature': X_train.columns,\n    'coefficient': log_reg.coef_[0]\n})\n\n# Sort by absolute value of coefficient\nimportance['abs_value'] = importance['coefficient'].abs()\nimportance = importance.sort_values(by='abs_value', ascending=False)\n\nimportance.head(15)\n\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:38.517346Z","iopub.execute_input":"2025-11-26T00:22:38.518157Z","iopub.status.idle":"2025-11-26T00:22:38.533369Z","shell.execute_reply.started":"2025-11-26T00:22:38.518125Z","shell.execute_reply":"2025-11-26T00:22:38.531974Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## 7.7 Plotting the feature","metadata":{}},{"cell_type":"code","source":"# 7.7 Plotting the feature\n\nplt.figure(figsize=(10,6))\nplt.barh(importance['feature'][:15], importance['abs_value'][:15])\nplt.gca().invert_yaxis()\nplt.title(\"Top 15 Most Important Features (Logistic Regression)\")\nplt.xlabel(\"Absolute Coefficient Value\")\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-26T00:22:43.17919Z","iopub.execute_input":"2025-11-26T00:22:43.179577Z","iopub.status.idle":"2025-11-26T00:22:43.455099Z","shell.execute_reply.started":"2025-11-26T00:22:43.179549Z","shell.execute_reply":"2025-11-26T00:22:43.453958Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"**Explanation:**\nLinear models don’t build decision trees. They don’t compute “importance scores. They learn weights (coefficients) that show:\n\nPositive → increases chance of fraud\n\nNegative → decreases chance of fraud\n\nLarger magnitude → more important\n\nIn summary, What does a positive and negative coefficient mean?\n\nPOSITIVE: A positive coefficient → increases the probability of fraud. Amount (+1.63) → Higher transaction amounts strongly increase the likelihood of fraud. This makes sense: fraudulent transactions often involve larger amounts. 22, V4, V1, V5, V28, V11. These PCA-transformed features (from the original anonymized dataset) contribute positively to the fraud probability.\n\nNEGATIVE: A negative coefficient → decreases probability of fraud. V14 (–1.57) V12 (–1.27) V10 (–1.14) V17, V20. These features make a transaction less likely to be fraud.\n\nSo, Logistic Regression identifies the most influential variables by the magnitude of their coefficients. Larger absolute values indicate stronger impact on the model’s prediction","metadata":{}},{"cell_type":"markdown","source":"# 8. CONCLUSION\nIn this project, I built an end-to-end machine learning pipeline to detect fraudulent credit card transactions. Using Logistic Regression, proper preprocessing, scaling, class-imbalance handling, and threshold tuning, the model achieved strong performance with high recall and F1-score for the fraud class. Feature analysis showed that Amount, V14, V12, and V10 were the most influential predictors. Overall, the pipeline follows the theory learned in class and demonstrates how data preparation, model training, and evaluation techniques work together to improve fraud detection.","metadata":{}}]}